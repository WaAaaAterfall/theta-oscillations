{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User settings\n",
    "\n",
    "---\n",
    "\n",
    "1) Keep the file structure like this:\n",
    "\n",
    "    normaliser_data/\n",
    "    \n",
    "        Subject01_YG/\n",
    "                HDF5/\n",
    "                    fp_deepinsight and trained model...\n",
    "                Intermediate_data/ \n",
    "                     intermediate .npy file & results\n",
    "                Plot/\n",
    "                    store the plot results\n",
    "                Recordings/\n",
    "                    ...\n",
    "                VideoGame/\n",
    "                    ...\n",
    "                Info/\n",
    "                    ...\n",
    "                    \n",
    "        Subject02_JY/\n",
    "                HDF5/\n",
    "                    fp_deepinsight and trained model...\n",
    "                Intermediate_data/ \n",
    "                     intermediate .npy file & results\n",
    "                Plot/\n",
    "                    store the plot results\n",
    "                Recordings/\n",
    "                    ...\n",
    "                VideoGame/\n",
    "                    ...\n",
    "                Info/\n",
    "                    ...\n",
    "                    \n",
    "                    \n",
    "2)  Then set the subject number to run this pipelineâ†“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "from skimage import io\n",
    "\n",
    "import mne\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "import h5py\n",
    "import numpy as np# Functions & Dataset\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import time\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_folder = '/Volumes/t7/dku2023/ieeg/'\n",
    "main_folder = 'F:/dku2023/ieeg/'\n",
    "\n",
    "file_paths = {}\n",
    "\n",
    "\n",
    "file_paths[1] = {}\n",
    "file_paths[1]['folder'] = main_folder+'normaliser_data/Subject01_YG/'\n",
    "file_paths[1]['recordings'] = main_folder+'normaliser_data/Subject01_YG/Recordings/yangge0108_navi.edf'\n",
    "file_paths[1]['ttl']        = main_folder+'normaliser_data/Subject01_YG/Recordings/Subject01_YG_part2_ttlMarkerNew.txt'\n",
    "file_paths[1]['game']       = main_folder+'normaliser_data/Subject01_YG/VideoGame/YG20210108Normaliser_navigation_20210108_142413.txt'\n",
    "file_paths[1]['info'] = {}\n",
    "file_paths[1]['info']['name'] = 'YG'\n",
    "\n",
    "file_paths[2] = {}\n",
    "file_paths[2]['folder'] = main_folder+'normaliser_data/Subject02_JY/'\n",
    "file_paths[2]['recordings'] = main_folder+'normaliser_data/Subject02_JY/Recordings/jinyong0118_navi.edf'\n",
    "file_paths[2]['ttl']        = main_folder+'normaliser_data/Subject02_JY/Recordings/Subject02_JY_ttlMarkerNew.txt'\n",
    "file_paths[2]['game']       = main_folder+'normaliser_data/Subject02_JY/VideoGame/Normaliser_navigation_20210118_144504.txt'\n",
    "file_paths[2]['info'] = {}\n",
    "file_paths[2]['info']['name'] = 'JY'\n",
    "\n",
    "file_paths[3] = {}\n",
    "file_paths[3]['folder'] = main_folder+'normaliser_data/Subject03_LYY/'\n",
    "file_paths[3]['recordings'] = main_folder+'normaliser_data/Subject03_LYY/Recordings/linyiyou031002.edf'\n",
    "file_paths[3]['ttl']        = main_folder+'normaliser_data/Subject03_LYY/Recordings/Subject03_LYY_ttlMarkerNew.txt'\n",
    "file_paths[3]['game']       = main_folder+'normaliser_data/Subject03_LYY/VideoGame/Normaliser_navigation_20210310_171054.txt'\n",
    "file_paths[3]['info'] = {}\n",
    "file_paths[3]['info']['name'] = 'LYY'\n",
    "\n",
    "file_paths[4] = {}\n",
    "file_paths[4]['folder'] = main_folder+'normaliser_data/Subject04_YY/'\n",
    "file_paths[4]['recordings'] = main_folder+'normaliser_data/Subject04_YY/Recordings/yuyue0504.edf'\n",
    "file_paths[4]['ttl']        = main_folder+'normaliser_data/Subject04_YY/Recordings/Subject04_YY_ttlMarkerNew.txt'\n",
    "file_paths[4]['game']       = main_folder+'normaliser_data/Subject04_YY/VideoGame/Normaliser_navigation_20210504_135840.txt'\n",
    "file_paths[4]['info'] = {}\n",
    "file_paths[4]['info']['name'] = 'YY'\n",
    "\n",
    "file_paths[6] = {}\n",
    "file_paths[6]['folder'] = main_folder+'normaliser_data/Subject06_ZQ/'\n",
    "file_paths[6]['recordings'] = main_folder+'normaliser_data/Subject06_ZQ/Recordings/zhengqiao0531.edf'\n",
    "file_paths[6]['ttl']        = main_folder+'normaliser_data/Subject06_ZQ/Recordings/Subject06_ZQ_ttlMarkerNew.txt'\n",
    "file_paths[6]['game']       = main_folder+'normaliser_data/Subject06_ZQ/VideoGame/Normaliser_navigation_20210531_095712Zhengqiao.txt'\n",
    "file_paths[6]['info'] = {}\n",
    "file_paths[6]['info']['name'] = 'ZQ'\n",
    "\n",
    "file_paths[7] = {}\n",
    "file_paths[7]['folder'] = main_folder+'normaliser_data/Subject07_CLC/'\n",
    "file_paths[7]['recordings'] = main_folder+'normaliser_data/Subject07_CLC/Recordings/chenlinchao07233.edf'\n",
    "file_paths[7]['ttl']        = main_folder+'normaliser_data/Subject07_CLC/Recordings/Subject07_CLC_ttlMarkerNew.txt'\n",
    "file_paths[7]['game']       = main_folder+'normaliser_data/Subject07_CLC/VideoGame/chenlinchao_Normaliser_navigation_20210723_123129.txt'\n",
    "file_paths[7]['info'] = {}\n",
    "file_paths[7]['info']['name'] = 'CLC'\n",
    "\n",
    "file_paths[8] = {}\n",
    "file_paths[8]['folder'] = main_folder+'normaliser_data/Subject08_SCM/'\n",
    "file_paths[8]['recordings'] = main_folder+'normaliser_data/Subject08_SCM/Recordings/shenchunmei07301.edf'\n",
    "file_paths[8]['ttl']        = main_folder+'normaliser_data/Subject08_SCM/Recordings/Subject08_SCM_ttlMarkerNew.txt'\n",
    "file_paths[8]['game']       = main_folder+'normaliser_data/Subject08_SCM/VideoGame/scm_Normaliser_navigation_20210730_151846.txt'\n",
    "file_paths[8]['info'] = {}\n",
    "file_paths[8]['info']['name'] = 'SCM'\n",
    "\n",
    "file_paths[9] = {}\n",
    "file_paths[9]['folder'] =main_folder+'normaliser_data/Subject09_LZ/'\n",
    "file_paths[9]['recordings'] = main_folder+'normaliser_data/Subject09_LZ/Recordings/liuzhe08251.edf'\n",
    "file_paths[9]['ttl']        = main_folder+'normaliser_data/Subject09_LZ/Recordings/Subject09_LZ_ttlMarkerNew.txt'\n",
    "file_paths[9]['game']       = main_folder+'normaliser_data/Subject09_LZ/VideoGame/Liuzhe_Normaliser_navigation_20210825_103719.txt'\n",
    "file_paths[9]['info'] = {}\n",
    "file_paths[9]['info']['name'] = 'LZ'\n",
    "\n",
    "file_paths[10] = {}\n",
    "file_paths[10]['folder'] = main_folder+'normaliser_data/Subject10_ZLX/'\n",
    "file_paths[10]['recordings'] = main_folder+'normaliser_data/Subject10_ZLX/Recordings/zhoulongxin09031.edf'\n",
    "file_paths[10]['ttl']        = main_folder+'normaliser_data/Subject10_ZLX/Recordings/Subject10_1_ttlMarkerNew.txt'\n",
    "file_paths[10]['game']       = main_folder+'normaliser_data/Subject10_ZLX/VideoGame/zhoulongxin_Normaliser_navigation_20210903_103923.txt'\n",
    "file_paths[10]['info'] = {}\n",
    "file_paths[10]['info']['name'] = 'ZLX'\n",
    "\n",
    "file_paths[11] = {}\n",
    "file_paths[11]['folder'] = main_folder+'normaliser_data/Subject11_WCF/'\n",
    "file_paths[11]['recordings'] = main_folder+'normaliser_data/Subject10_ZLX/Recordings/wangchunfei0927.edf'\n",
    "file_paths[11]['ttl']        = main_folder+'normaliser_data/Subject10_ZLX/Recordings/Subject11_ttlMarkerNew.txt'\n",
    "file_paths[11]['game']       = main_folder+'normaliser_data/Subject10_ZLX/VideoGame/wangchunfei_Normaliser_navigation_20210927_161155.txt'\n",
    "file_paths[11]['info'] = {}\n",
    "file_paths[11]['info']['name'] = 'ZLX'\n",
    "hpc_chan2 = {}\n",
    "\n",
    "hpc_chan2[1] = ['POL B1', 'POL N1']\n",
    "hpc_chan2[2] = ['POL B1', 'POL D3']\n",
    "hpc_chan2[3] = ['POL E1']\n",
    "hpc_chan2[4] = ['POL I2']\n",
    "hpc_chan2[7] = ['POL I3']\n",
    "hpc_chan2[8] = ['POL J1','EEG C1-Ref']\n",
    "hpc_chan2[9] = ['EEG A1-Ref','POL B2']\n",
    "hpc_chan2[10] = ['POL K1','EEG A1-Ref']\n",
    "\n",
    "### Use this selection for bipolarizing\n",
    "# hpc_chan2[1] = ['POL B1','POL B2']\n",
    "# hpc_chan2[2] = ['POL B1', 'POL D3']\n",
    "# hpc_chan2[3] = ['POL E1','POL E2']\n",
    "# hpc_chan2[4] = ['POL I2','POL I3']\n",
    "# hpc_chan2[7] = ['POL I3', 'POL I4']\n",
    "# hpc_chan2[8] = ['POL J1','POL J2']\n",
    "\n",
    "# deepInsight_encode =  file_paths[sub]['folder'] + 'HDF5/deepInsight_encode.h5' \n",
    "# fp_plot = file_paths[sub]['folder'] + 'Plot/'\n",
    "# fp_data = file_paths[sub]['folder'] + 'Intermediate_data/'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## necessary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and parsing each trial video game feature\n",
    "def structure_gamelog(gamelog):\n",
    "\n",
    "    trials_data = {}\n",
    "\n",
    "    reading_navigation_positions = False\n",
    "\n",
    "    for line in gamelog:\n",
    "\n",
    "\n",
    "        if line.startswith('Trial ended'):\n",
    "            trials_data[trial_n]['Trial_ended_time'] = float(line.split(' ')[-1]) \n",
    "\n",
    "            trial_duration = trials_data[trial_n]['Trial_ended_time'] - trials_data[trial_n]['Trial_start_time']\n",
    "            trials_data[trial_n]['trial_duration'] = trial_duration\n",
    "            trials_data[trial_n]['n_of_objects'] = len(trials_data[trial_n]['Objects_location'])\n",
    "\n",
    "            trials_data[trial_n]['Navigation'] = np.array(trials_data[trial_n]['Navigation'])\n",
    "            \n",
    "            ### Compute velocity from position (navigation) coordinates\n",
    "            trial_duration = trials_data[trial_n]['Navigation'][:,0][-1] - trials_data[trial_n]['Navigation'][:,0][0]\n",
    "            # trial_distance in meters\n",
    "            trial_distance = np.sum(np.abs(np.diff(trials_data[trial_n]['Navigation'][:,1] +1j* trials_data[trial_n]['Navigation'][:,2])))\n",
    "            trials_data[trial_n]['Velocity'] = trial_distance/trial_duration\n",
    "            \n",
    "            \n",
    "            trials_data[trial_n]['Objects_location'] = np.array(trials_data[trial_n]['Objects_location'])\n",
    "\n",
    "            # trial_distance in angles\n",
    "            trial_distance = np.unwrap( np.angle( trials_data[trial_n]['Navigation'][:,1] +1j* trials_data[trial_n]['Navigation'][:,2] ) )\n",
    "            trial_distance = trial_distance[-1] - trial_distance[0]\n",
    "            trials_data[trial_n]['Trial_distance'] = trial_distance\n",
    "\n",
    "            ### cue/sec needs to get how many laps were performed and ajust it to the number of items per every lap (2pi)\n",
    "            this_trial_crossed_objects = trial_distance  * trials_data[trial_n]['n_of_objects'] / np.pi*2\n",
    "            trial_cue_per_sec = trials_data[trial_n]['trial_duration'] / this_trial_crossed_objects\n",
    "            trials_data[trial_n]['trial_cue_per_sec'] = trial_cue_per_sec\n",
    "\n",
    "\n",
    "            reading_navigation_positions = False\n",
    "\n",
    "\n",
    "        if reading_navigation_positions==False:\n",
    "\n",
    "            if line.startswith('Trial number'): \n",
    "                trial_n = int(line.split(': ')[1][:-1])\n",
    "                trials_data[trial_n] = {}\n",
    "            if line.startswith('Speed'): \n",
    "                trials_data[trial_n]['Speed'] = float(line.split('Speed: ')[1][:-1])\n",
    "            if line.startswith('Ring_size'): \n",
    "                trials_data[trial_n]['Ring_size'] = float(line.split('Ring_size: ')[1][:-1])\n",
    "\n",
    "            if line.startswith('Objects location'):\n",
    "                ### actualItem  +\" \"+ itemX +\" \"+ itemZ +\" \"+ itemAngle\n",
    "                trials_data[trial_n]['Objects_location'] = []\n",
    "            if line.startswith('Item:'):\n",
    "                trials_data[trial_n]['Objects_location'].append( np.array(line.split('Item: ')[1][:-1].split(' ')).astype(float) )\n",
    "\n",
    "            if line.startswith('Trial start'):\n",
    "                trials_data[trial_n]['Navigation'] = []\n",
    "                reading_navigation_positions = True\n",
    "                trials_data[trial_n]['Trial_start_time'] = float(line.split(' ')[-1]) \n",
    "\n",
    "\n",
    "            if line.startswith('Question onset'):\n",
    "                trials_data[trial_n]['Testing'] = {}\n",
    "                trials_data[trial_n]['Testing']['Question_onset'] = float(line.split('Question onset ')[1][:-1])\n",
    "\n",
    "            if line.startswith('Testing'):            \n",
    "                tmp_line = line[:-1].split(' ')\n",
    "                trials_data[trial_n]['Testing']['response_time'] = float(tmp_line[1])\n",
    "                trials_data[trial_n]['Testing']['cued_object'] = int(tmp_line[3])\n",
    "                trials_data[trial_n]['Testing']['option_1'] = int(tmp_line[5])\n",
    "                trials_data[trial_n]['Testing']['option_2'] = int(tmp_line[7])\n",
    "                trials_data[trial_n]['Testing']['asnwered_object'] = int(tmp_line[9])\n",
    "                trials_data[trial_n]['Testing']['asnwered_key'] = tmp_line[11]\n",
    "\n",
    "        elif reading_navigation_positions==True:\n",
    "            trials_data[trial_n]['Navigation'].append( np.array(line[:-1].split(' ')).astype(float) )\n",
    "\n",
    "    return trials_data\n",
    "\n",
    "\n",
    "def interp_f(behavioural_signal, lfp_size):\n",
    "    n_samples = len(behavioural_signal)\n",
    "    x = np.linspace(0, n_samples, num=n_samples, endpoint=True)\n",
    "    interpolate_f = interp1d(x, behavioural_signal, kind='linear')\n",
    "    new_sampling_space = np.linspace(0, n_samples, num=lfp_size, endpoint=True)\n",
    "    return interpolate_f(new_sampling_space)\n",
    "\n",
    "\n",
    "def get_behavioral_position(trials_data, trial_number):\n",
    "    position_array = []\n",
    "    for count in range(len(trials_data[trial_number]['Navigation'])):\n",
    "        time = trials_data[trial_number]['Navigation'][count][0]\n",
    "        x = trials_data[trial_number]['Navigation'][count][1]\n",
    "        y = trials_data[trial_number]['Navigation'][count][2]\n",
    "        position_array.append([trial_number, time, x, y])\n",
    "        \n",
    "    return np.array(position_array)\n",
    "\n",
    "\n",
    "'''\n",
    "Extract navigation data from recording txt file\n",
    "Return:\n",
    "    data selected from hippocampol channel, \n",
    "    lfp_theta_filtered, applied norch and theta filter on raw data\n",
    "    trials_data, extract from game log, see def struct_gamelog\n",
    "    event_samples, TTL information and index for navigation start, end...etc\n",
    "    int(raw.info['sfreq']) s_frequency info\n",
    "'''\n",
    "def get_subject_data(sub, file_paths, hpc_chan2):\n",
    "\n",
    "    # Load brain recordings\n",
    "    raw = mne.io.read_raw_edf(file_paths[sub]['recordings'])\n",
    "    print(\"all channel name:\", raw.ch_names)\n",
    "    #print(np.shape(raw))\n",
    "    # Load TTL events\n",
    "    TTL = pd.read_csv(file_paths[sub]['ttl'],\n",
    "                      sep=' ',\n",
    "                      names=['time', 'type', 'n'])\n",
    "    #print(TTL)\n",
    "\n",
    "    # Get hippocampal channels of this subject\n",
    "    sub_hpc_chans = hpc_chan2[sub]\n",
    "    data = raw.get_data(picks=sub_hpc_chans)\n",
    "\n",
    "    # Load videoGame data and structure it by trials\n",
    "    gamelog = []\n",
    "    with open(file_paths[sub]['game'], 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            gamelog.append(line)\n",
    "    trials_data = structure_gamelog(gamelog)\n",
    "\n",
    "    ### Load the signal\n",
    "    # Here loading 2 signals in the Amygdala. This will be specific for each patient.\n",
    "    lfps = raw.get_data(picks=sub_hpc_chans)\n",
    "\n",
    "    ## Notch is done to remove the noise from the power line in the building. Since China operates on a 220V voltage and 50Hz and remove the 50Hz and its harmonics (100, 150, etc) to the signal.\n",
    "    lfp_bp_notch = mne.filter.notch_filter(lfps,\n",
    "                                           raw.info['sfreq'],\n",
    "                                           [50., 100., 150., 200.],\n",
    "                                           notch_widths=.1)\n",
    "#   lfp_bp_notch = lfp_bp_notch[0] - lfp_bp_notch[1]\n",
    "    low_freq = 2\n",
    "    high_freq = 10\n",
    "\n",
    "    lfp_theta_filtered = mne.filter.filter_data(lfp_bp_notch, int(raw.info['sfreq']), low_freq, high_freq, verbose=False )\n",
    "\n",
    "    ### TTL markers\n",
    "    event_samples = np.vstack((\n",
    "        TTL[TTL['type'] == 2]['time'].values,\n",
    "        TTL[TTL['type'] == 3]['time'].values,\n",
    "        TTL[TTL['type'] == 4]['time'].values,\n",
    "        TTL[TTL['type'] == 5]['time'].values,\n",
    "    )).T\n",
    "\n",
    "    return data, lfp_theta_filtered, trials_data, event_samples, int(raw.info['sfreq'])\n",
    "\n",
    "\n",
    "'''\n",
    "Deal with ieeg signal data\n",
    "Return: \n",
    "a dict with essential information\n",
    "physilogical data: a list of the ieeg signal between navigation for all trials\n",
    "timestamps: a list of index matching physilogical data\n",
    "position_2d: coordinate..ish matching each timestamp\n",
    "event_marker: \n",
    "'''\n",
    "def get_preprocess_data(trials_data,markers,physilogical_data,start_end_indice=[0,1],test=False): #trials, event_sample, notch_filtered_data\n",
    "    \n",
    "    '''\n",
    "    start_end_indice:\n",
    "        markers[:][0] the start of one trial\n",
    "        markers[:][1] the end of one trial\n",
    "        markers[:][2] the onset of the testing question\n",
    "        markers[:][3] the onset of response\n",
    "    test: \n",
    "        True: decode the recall part, no real output(position)\n",
    "    '''\n",
    "    physilogical_data_navi= {}\n",
    "\n",
    "    for chan in range(physilogical_data.shape[0]):\n",
    "        \n",
    "        for trial_num in range(min(len(trials_data), len(markers))):  \n",
    "            behavioural_signal = get_behavioral_position(trials_data, trial_num)\n",
    "            navi_start = int(markers[trial_num][start_end_indice[0]])\n",
    "            navi_stop = int(markers[trial_num][start_end_indice[1]])\n",
    "            lfp_size = abs(navi_stop - navi_start)\n",
    "            \n",
    "            if (chan == 0) & (trial_num == 0):\n",
    "                start_array = np.array([lfp_size])\n",
    "                timestamps = np.linspace(navi_start,navi_stop-1,lfp_size, dtype=int)\n",
    "                if test == False :\n",
    "                    position_2d = np.vstack(( interp_f(behavioural_signal.T[2], lfp_size) , interp_f(behavioural_signal.T[3], lfp_size)))\n",
    "                \n",
    "            if (chan == 0) & (trial_num > 0):\n",
    "                start_array = np.hstack((start_array,np.array([int(start_array[trial_num-1])+lfp_size])))\n",
    "                timestamps = np.hstack(( timestamps, np.linspace(navi_start,navi_stop-1,lfp_size, dtype=int) ))\n",
    "                if test == False :\n",
    "                    position_2d = np.hstack((position_2d, np.vstack(( interp_f(behavioural_signal.T[2], lfp_size) , interp_f(behavioural_signal.T[3], lfp_size)))  ))\n",
    "                \n",
    "            if trial_num == 0:\n",
    "                physilogical_data_navi[chan]= physilogical_data[chan][navi_start:navi_stop]\n",
    "                \n",
    "            if trial_num > 0:\n",
    "                physilogical_data_navi[chan]  = np.hstack((physilogical_data_navi[chan], physilogical_data[chan][navi_start:navi_stop] ))\n",
    "    \n",
    "    print(np.shape(timestamps))\n",
    "    print(np.shape(start_array))\n",
    "    print(np.shape(np.array([physilogical_data_navi[i] for i in range(len(physilogical_data_navi))]).T))\n",
    "    if test == True :\n",
    "        return {\n",
    "            'physilogical_data': np.array([physilogical_data_navi[i] for i in range(len(physilogical_data_navi))]).T,\n",
    "            'timestamps': timestamps,\n",
    "            'event_marker_each_trial': start_array\n",
    "        }\n",
    "    if test == False :               \n",
    "        return {\n",
    "            'physilogical_data': np.array([physilogical_data_navi[i] for i in range(len(physilogical_data_navi))]).T,\n",
    "            'timestamps': timestamps,\n",
    "            'position_2d': position_2d,\n",
    "            'event_marker_each_trial': start_array\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Number of Theta Cycles from physilogical data nad event sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 10\n",
    "raw, lfp_theta_filtered, trials_data, event_samples, sfreq = get_subject_data(sub, file_paths, hpc_chan2)\n",
    "print(min(len(event_samples), len(trials_data)))\n",
    "preprocess_data = get_preprocess_data(trials_data,event_samples,lfp_theta_filtered)\n",
    "\n",
    "raw_data = preprocess_data['physilogical_data']\n",
    "timestamps = preprocess_data['timestamps']\n",
    "\n",
    "print(np.shape(raw_data))\n",
    "print(np.shape(timestamps))\n",
    "\n",
    "#Set trial number\n",
    "trial_total_num = min(len(trials_data), len(event_samples))\n",
    "channel_num = np.shape(raw_data)[1]\n",
    "theta_cycle_record = {}\n",
    "theta_cycle_record['Speed'] = []\n",
    "theta_cycle_record['Ring_size'] = []\n",
    "theta_cycle_record['cue_per_sec'] = []\n",
    "#get timestamp for trial to start and to end\n",
    "for channel in range(channel_num):\n",
    "    theta_cycle_record[hpc_chan2[sub][channel]] = []\n",
    "    for trial_num in range(trial_total_num):\n",
    "\n",
    "        navi_start = int(event_samples[trial_num][0])\n",
    "        navi_end = int(event_samples[trial_num][1])\n",
    "\n",
    "        #Find the index in the array that match the timestamps\n",
    "        start_index = np.where(timestamps == navi_start)[0][0] if np.any(timestamps == navi_start) else None\n",
    "        end_index = np.where(timestamps == navi_end - 1)[0][0] if np.any(timestamps == navi_end - 1) else None\n",
    "        assert(timestamps[start_index] == navi_start)\n",
    "        physilogical_data_navigation = raw_data[start_index:end_index + 1, channel]\n",
    "        count = 0\n",
    "        index = 0\n",
    "        while index < np.shape(physilogical_data_navigation)[0] - 1:\n",
    "            if physilogical_data_navigation[index] * physilogical_data_navigation[index + 1] < 0:\n",
    "                count += 1\n",
    "                index += 1\n",
    "            elif physilogical_data_navigation[index] * physilogical_data_navigation[index + 1] > 0:\n",
    "                index += 1\n",
    "            else:\n",
    "                temp_index = index + 1\n",
    "                while physilogical_data_navigation[index] * physilogical_data_navigation[temp_index + 1] == 0:\n",
    "                    temp_index += 1\n",
    "                if physilogical_data_navigation[index] * physilogical_data_navigation[temp_index + 1] < 0:\n",
    "                    count += 1\n",
    "                index = temp_index\n",
    "        theta_cycle_record[hpc_chan2[sub][channel]].append(count)\n",
    "\n",
    "for trial_num in range(trial_total_num):\n",
    "    theta_cycle_record['Speed'].append(trials_data[trial_num]['Speed'])\n",
    "    theta_cycle_record['Ring_size'].append(trials_data[trial_num]['Ring_size'])\n",
    "    theta_cycle_record['cue_per_sec'].append(trials_data[trial_num]['Ring_size'] / trials_data[trial_num]['Speed'])\n",
    "\n",
    "\n",
    "print(theta_cycle_record)\n",
    "pd.DataFrame(theta_cycle_record).to_csv(f\"theta_cycles/theta_cycle_{sub}_{file_paths[sub]['info']['name']}.csv\", index = False)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
